Create a model response
POST
 
https://api.openai.com/v1/responses
Creates a model response. Provide text or image inputs to generate text or JSON outputs. Have the model call your own custom code or use built-in tools like web search or file search to use your own data as input for the model's response.
Request body

background
boolean or null
Optional
Defaults to false
Whether to run the model response in the background. Learn more.
include
array or null
Optional
Specify additional output data to include in the model response. Currently supported values are:

code_interpreter_call.outputs: Includes the outputs of python code execution in code interpreter tool call items.
computer_call_output.output.image_url: Include image urls from the computer call output.
file_search_call.results: Include the search results of the file search tool call.
message.input_image.image_url: Include image urls from the input message.
message.output_text.logprobs: Include logprobs with assistant messages.
reasoning.encrypted_content: Includes an encrypted version of reasoning tokens in reasoning item outputs. This enables reasoning items to be used in multi-turn conversations when using the Responses API statelessly (like when the store parameter is set to false, or when an organization is enrolled in the zero data retention program).
input
string or array
Optional
Text, image, or file inputs to the model, used to generate a response.

Learn more:

Text inputs and outputs
Image inputs
File inputs
Conversation state
Function calling

Show possible types
instructions
string or null
Optional
A system (or developer) message inserted into the model's context.

When using along with previous_response_id, the instructions from a previous response will not be carried over to the next response. This makes it simple to swap out system (or developer) messages in new responses.
max_output_tokens
integer or null
Optional
An upper bound for the number of tokens that can be generated for a response, including visible output tokens and reasoning tokens.
max_tool_calls
integer or null
Optional
The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.
metadata
map
Optional
Set of 16 key-value pairs that can be attached to an object. This can be useful for storing additional information about the object in a structured format, and querying for objects via API or the dashboard.

Keys are strings with a maximum length of 64 characters. Values are strings with a maximum length of 512 characters.
model
string
Optional
Model ID used to generate the response, like gpt-4o or o3. OpenAI offers a wide range of models with different capabilities, performance characteristics, and price points. Refer to the model guide to browse and compare available models.
parallel_tool_calls
boolean or null
Optional
Defaults to true
Whether to allow the model to run tool calls in parallel.
previous_response_id
string or null
Optional
The unique ID of the previous response to the model. Use this to create multi-turn conversations. Learn more about conversation state.
prompt
object or null
Optional
Reference to a prompt template and its variables. Learn more.

Hide properties
id
string
Required
The unique identifier of the prompt template to use.
variables
map
Optional
Optional map of values to substitute in for variables in your prompt. The substitution values can either be strings, or other Response input types like images or files.
version
string or null
Optional
Optional version of the prompt template.
reasoning
object or null
Optional
o-series models only

Configuration options for reasoning models.

Hide properties
effort
string or null
Optional
Defaults to medium
o-series models only

Constrains effort on reasoning for reasoning models. Currently supported values are low, medium, and high. Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.
generate_summary
Deprecated
string or null
Optional
Deprecated: use summary instead.

A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of auto, concise, or detailed.
summary
string or null
Optional
A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of auto, concise, or detailed.
service_tier
string or null
Optional
Defaults to auto
Specifies the processing type used for serving the request.

If set to 'auto', then the request will be processed with the service tier configured in the Project settings. Unless otherwise configured, the Project will use 'default'.
If set to 'default', then the requset will be processed with the standard pricing and performance for the selected model.
If set to 'flex' or 'priority', then the request will be processed with the corresponding service tier. Contact sales to learn more about Priority processing.
When not set, the default behavior is 'auto'.
When the service_tier parameter is set, the response body will include the service_tier value based on the processing mode actually used to serve the request. This response value may be different from the value set in the parameter.
store
boolean or null
Optional
Defaults to true
Whether to store the generated model response for later retrieval via API.
stream
boolean or null
Optional
Defaults to false
If set to true, the model response data will be streamed to the client as it is generated using server-sent events. See the Streaming section below for more information.
temperature
number or null
Optional
Defaults to 1
What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or top_p but not both.
text
object
Optional
Configuration options for a text response from the model. Can be plain text or structured JSON data. Learn more:

Text inputs and outputs
Structured Outputs

Hide properties
format
object
Optional
An object specifying the format that the model must output.

Configuring { "type": "json_schema" } enables Structured Outputs, which ensures the model will match your supplied JSON schema. Learn more in the Structured Outputs guide.

The default format is { "type": "text" } with no additional options.

Not recommended for gpt-4o and newer models:

Setting to { "type": "json_object" } enables the older JSON mode, which ensures the message the model generates is valid JSON. Using json_schema is preferred for models that support it.

Hide possible types
Text
object
Default response format. Used to generate text responses.

Hide properties
type
string
Required
The type of response format being defined. Always text.
JSON schema
object
JSON Schema response format. Used to generate structured JSON responses. Learn more about Structured Outputs.

Hide properties
name
string
Required
The name of the response format. Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a maximum length of 64.
schema
object
Required
The schema for the response format, described as a JSON Schema object. Learn how to build JSON schemas here.
type
string
Required
The type of response format being defined. Always json_schema.
description
string
Optional
A description of what the response format is for, used by the model to determine how to respond in the format.
strict
boolean or null
Optional
Defaults to false
Whether to enable strict schema adherence when generating the output. If set to true, the model will always follow the exact schema defined in the schema field. Only a subset of JSON Schema is supported when strict is true. To learn more, read the Structured Outputs guide.
JSON object
object
JSON object response format. An older method of generating JSON responses. Using json_schema is recommended for models that support it. Note that the model will not generate JSON without a system or user message instructing it to do so.

Hide properties
type
string
Required
The type of response format being defined. Always json_object.
tool_choice
string or object
Optional
How the model should select which tool (or tools) to use when generating a response. See the tools parameter to see how to specify which tools the model can call.

Hide possible types
Tool choice mode
string
Controls which (if any) tool is called by the model.

none means the model will not call any tool and instead generates a message.

auto means the model can pick between generating a message or calling one or more tools.

required means the model must call one or more tools.
Hosted tool
object
Indicates that the model should use a built-in tool to generate a response. Learn more about built-in tools.

Hide properties
type
string
Required
The type of hosted tool the model should to use. Learn more about built-in tools.

Allowed values are:

file_search
web_search_preview
computer_use_preview
code_interpreter
image_generation
Function tool
object
Use this option to force the model to call a specific function.

Hide properties
name
string
Required
The name of the function to call.
type
string
Required
For function calling, the type is always function.
MCP tool
object
Use this option to force the model to call a specific tool on a remote MCP server.

Hide properties
server_label
string
Required
The label of the MCP server to use.
type
string
Required
For MCP tools, the type is always mcp.
name
string or null
Optional
The name of the tool to call on the server.
tools
array
Optional
An array of tools the model may call while generating a response. You can specify which tool to use by setting the tool_choice parameter.

The two categories of tools you can provide the model are:

Built-in tools: Tools that are provided by OpenAI that extend the model's capabilities, like web search or file search. Learn more about built-in tools.
Function calls (custom tools): Functions that are defined by you, enabling the model to call your own code. Learn more about function calling.

Hide possible types
Function
object
Defines a function in your own code the model can choose to call. Learn more about function calling.

Hide properties
name
string
Required
The name of the function to call.
parameters
object
Required
A JSON schema object describing the parameters of the function.
strict
boolean
Required
Whether to enforce strict parameter validation. Default true.
type
string
Required
The type of the function tool. Always function.
description
string
Optional
A description of the function. Used by the model to determine whether or not to call the function.
File search
object
A tool that searches for relevant content from uploaded files. Learn more about the file search tool.

Hide properties
type
string
Required
The type of the file search tool. Always file_search.
vector_store_ids
array
Required
The IDs of the vector stores to search.
filters
object
Optional
A filter to apply.

Hide possible types
Comparison Filter
object
A filter used to compare a specified attribute key to a given value using a defined comparison operation.

Hide properties
key
string
Required
The key to compare against the value.
type
string
Required
Specifies the comparison operator: eq, ne, gt, gte, lt, lte.

eq: equals
ne: not equal
gt: greater than
gte: greater than or equal
lt: less than
lte: less than or equal
value
string / number / boolean
Required
The value to compare against the attribute key; supports string, number, or boolean types.
Compound Filter
object
Combine multiple filters using and or or.

Hide properties
filters
array
Required
Array of filters to combine. Items can be ComparisonFilter or CompoundFilter.

Show possible types
type
string
Required
Type of operation: and or or.
max_num_results
integer
Optional
The maximum number of results to return. This number should be between 1 and 50 inclusive.
ranking_options
object
Optional
Ranking options for search.

Hide properties
ranker
string
Optional
The ranker to use for the file search.
score_threshold
number
Optional
The score threshold for the file search, a number between 0 and 1. Numbers closer to 1 will attempt to return only the most relevant results, but may return fewer results.
Web search preview
object
This tool searches the web for relevant results to use in a response. Learn more about the web search tool.

Hide properties
type
string
Required
The type of the web search tool. One of web_search_preview or web_search_preview_2025_03_11.
search_context_size
string
Optional
High level guidance for the amount of context window space to use for the search. One of low, medium, or high. medium is the default.
user_location
object
Optional
The user's location.

Hide properties
type
string
Required
The type of location approximation. Always approximate.
city
string
Optional
Free text input for the city of the user, e.g. San Francisco.
country
string
Optional
The two-letter ISO country code of the user, e.g. US.
region
string
Optional
Free text input for the region of the user, e.g. California.
timezone
string
Optional
The IANA timezone of the user, e.g. America/Los_Angeles.
Computer use preview
object
A tool that controls a virtual computer. Learn more about the computer tool.

Hide properties
display_height
integer
Required
The height of the computer display.
display_width
integer
Required
The width of the computer display.
environment
string
Required
The type of computer environment to control.
type
string
Required
The type of the computer use tool. Always computer_use_preview.
MCP tool
object
Give the model access to additional tools via remote Model Context Protocol (MCP) servers. Learn more about MCP.

Hide properties
server_label
string
Required
A label for this MCP server, used to identify it in tool calls.
server_url
string
Required
The URL for the MCP server.
type
string
Required
The type of the MCP tool. Always mcp.
allowed_tools
array or object
Optional
List of allowed tool names or a filter object.

Hide possible types
MCP allowed tools
array
A string array of allowed tool names
MCP allowed tools filter
object
A filter object to specify which tools are allowed.

Hide properties
tool_names
array
Optional
List of allowed tool names.
headers
object or null
Optional
Optional HTTP headers to send to the MCP server. Use for authentication or other purposes.
require_approval
object or string
Optional
Defaults to always
Specify which of the MCP server's tools require approval.

Hide possible types
MCP tool approval filter
object

Hide properties
always
object
Optional
A list of tools that always require approval.

Hide properties
tool_names
array
Optional
List of tools that require approval.
never
object
Optional
A list of tools that never require approval.

Hide properties
tool_names
array
Optional
List of tools that do not require approval.
MCP tool approval setting
string
Specify a single approval policy for all tools. One of always or never. When set to always, all tools will require approval. When set to never, all tools will not require approval.
server_description
string
Optional
Optional description of the MCP server, used to provide more context.
Code interpreter
object
A tool that runs Python code to help generate a response to a prompt.

Hide properties
container
string or object
Required
The code interpreter container. Can be a container ID or an object that specifies uploaded file IDs to make available to your code.

Hide possible types
string
The container ID.
CodeInterpreterContainerAuto
object
Configuration for a code interpreter container. Optionally specify the IDs of the files to run the code on.

Hide properties
type
string
Required
Always auto.
file_ids
array
Optional
An optional list of uploaded files to make available to your code.
type
string
Required
The type of the code interpreter tool. Always code_interpreter.
Image generation tool
object
A tool that generates images using a model like gpt-image-1.

Hide properties
type
string
Required
The type of the image generation tool. Always image_generation.
background
string
Optional
Defaults to auto
Background type for the generated image. One of transparent, opaque, or auto. Default: auto.
input_image_mask
object
Optional
Optional mask for inpainting. Contains image_url (string, optional) and file_id (string, optional).

Hide properties
file_id
string
Optional
File ID for the mask image.
image_url
string
Optional
Base64-encoded mask image.
model
string
Optional
Defaults to gpt-image-1
The image generation model to use. Default: gpt-image-1.
moderation
string
Optional
Defaults to auto
Moderation level for the generated image. Default: auto.
output_compression
integer
Optional
Defaults to 100
Compression level for the output image. Default: 100.
output_format
string
Optional
Defaults to png
The output format of the generated image. One of png, webp, or jpeg. Default: png.
partial_images
integer
Optional
Defaults to 0
Number of partial images to generate in streaming mode, from 0 (default value) to 3.
quality
string
Optional
Defaults to auto
The quality of the generated image. One of low, medium, high, or auto. Default: auto.
size
string
Optional
Defaults to auto
The size of the generated image. One of 1024x1024, 1024x1536, 1536x1024, or auto. Default: auto.
Local shell tool
object
A tool that allows the model to execute shell commands in a local environment.

Hide properties
type
string
Required
The type of the local shell tool. Always local_shell.
top_logprobs
integer or null
Optional
An integer between 0 and 20 specifying the number of most likely tokens to return at each token position, each with an associated log probability.
top_p
number or null
Optional
Defaults to 1
An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.

We generally recommend altering this or temperature but not both.
truncation
string or null
Optional
Defaults to disabled
The truncation strategy to use for the model response.

auto: If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation.
disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error.
user
string
Optional
A stable identifier for your end-users. Used to boost cache hit rates by better bucketing similar requests and to help OpenAI detect and prevent abuse. Learn more.
Returns

Returns a Response object.


Text input

Image input

Web search

File search

Streaming

Functions

Reasoning
Example request
curl https://api.openai.com/v1/responses \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $OPENAI_API_KEY" \
  -d '{
    "model": "gpt-4.1",
    "input": "Tell me a three sentence bedtime story about a unicorn."
  }'
Response
{
  "id": "resp_67ccd2bed1ec8190b14f964abc0542670bb6a6b452d3795b",
  "object": "response",
  "created_at": 1741476542,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "model": "gpt-4.1-2025-04-14",
  "output": [
    {
      "type": "message",
      "id": "msg_67ccd2bf17f0819081ff3bb2cf6508e60bb6a6b452d3795b",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "In a peaceful grove beneath a silver moon, a unicorn named Lumina discovered a hidden pool that reflected the stars. As she dipped her horn into the water, the pool began to shimmer, revealing a pathway to a magical realm of endless night skies. Filled with wonder, Lumina whispered a wish for all who dream to find their own hidden magic, and as she glanced back, her hoofprints sparkled like stardust.",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 36,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 87,
    "output_tokens_details": {
      "reasoning_tokens": 0
    },
    "total_tokens": 123
  },
  "user": null,
  "metadata": {}
}
Get a model response
GET
 
https://api.openai.com/v1/responses/{response_id}
Retrieves a model response with the given ID.
Path parameters

response_id
string
Required
The ID of the response to retrieve.
Query parameters

include
array
Optional
Additional fields to include in the response. See the include parameter for Response creation above for more information.
starting_after
integer
Optional
The sequence number of the event after which to start streaming.
stream
boolean
Optional
If set to true, the model response data will be streamed to the client as it is generated using server-sent events. See the Streaming section below for more information.
Returns

The Response object matching the specified ID.

Example request
curl https://api.openai.com/v1/responses/resp_123 \
    -H "Content-Type: application/json" \
    -H "Authorization: Bearer $OPENAI_API_KEY"
Response
{
  "id": "resp_67cb71b351908190a308f3859487620d06981a8637e6bc44",
  "object": "response",
  "created_at": 1741386163,
  "status": "completed",
  "error": null,
  "incomplete_details": null,
  "instructions": null,
  "max_output_tokens": null,
  "model": "gpt-4o-2024-08-06",
  "output": [
    {
      "type": "message",
      "id": "msg_67cb71b3c2b0819084d481baaaf148f206981a8637e6bc44",
      "status": "completed",
      "role": "assistant",
      "content": [
        {
          "type": "output_text",
          "text": "Silent circuits hum,  \nThoughts emerge in data streams—  \nDigital dawn breaks.",
          "annotations": []
        }
      ]
    }
  ],
  "parallel_tool_calls": true,
  "previous_response_id": null,
  "reasoning": {
    "effort": null,
    "summary": null
  },
  "store": true,
  "temperature": 1.0,
  "text": {
    "format": {
      "type": "text"
    }
  },
  "tool_choice": "auto",
  "tools": [],
  "top_p": 1.0,
  "truncation": "disabled",
  "usage": {
    "input_tokens": 32,
    "input_tokens_details": {
      "cached_tokens": 0
    },
    "output_tokens": 18,